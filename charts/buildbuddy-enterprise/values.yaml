## Default values for BuildBuddy Enterprise.
## This is a YAML-formatted file.
## Declare variables to be passed into your templates.

## The BuildBuddy container image to use.
image:
  repository: gcr.io/flame-public/buildbuddy-app-enterprise
  tag: enterprise-v2.65.0
  imagePullPolicy: IfNotPresent

## The number of app replicas to run.
## If setting this number higher than 1, make sure to configure a mysql database (not sqlite) and gcs/s3 storage
## so data can be shared across replicas.
## Using multiple replicas with a sqlite3 database or disk storage is not supported.
replicas: 1

## The (optional) nodeSelector used for placing executors and apps in separate node pools/groups (useful when autoscaling executors)
# nodeSelector:
#   # GCP
#   cloud.google.com/gke-nodepool: my-app-pool
#   # AWS
#   eks.amazonaws.com/nodegroup: my-app-pool

## The taints to schedule pods on (useful for mixed architecture clusters)
# tolerations:
#   - effect: NoSchedule
#     key: hardware_requirements
#     operator: Equal
#     value: x86_64

## The default resources to give the app.
resources:
  limits:
    cpu: "2"
    memory: "8Gi"
  requests:
    cpu: "1"
    memory: "4Gi"

## Arguments to pass directly to the buildbuddy binary
#args:
#  - "--auto_migrate_db=false"

# Enables distributed disk caching configuration
distributed:
  enabled: false
  # Per-replica disk
  size: 100Gi
  # Enable the use of pd-ssd on GCP
  # ssd: true
  # Configure alternative storage class name (use either storageClass OR ssd, not both)
  # storageClass: true
  podManagementPolicy: "Parallel"

## Enables disk storage using Persistent Volume Claims.
## You'll also need to specify a disk block in the config cache and/or storage blocks below.
## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
## ref: https://docs.gitlab.com/ee/install/requirements.html#storage
disk:
  ## This volume persists build events, cache artifacts, etc. when BuildBuddy is
  ## configured to use disk storage. To use this disk in your config, make sure
  ## your root_directory options begin with with /data/.
  data:
    enabled: true
    size: 100Gi
    ## If defined, volume.beta.kubernetes.io/storage-class: <storageClass>
    ## Default: volume.alpha.kubernetes.io/storage-class: default
    ##
    # storageClass:
    accessMode: ReadWriteOnce
  # existingClaim: ""

## Exposes the BuildBuddy service via a LoadBalancer on the given ports.
service:
  type: LoadBalancer

  externalHTTPPort: 80
  externalGRPCPort: 1985
  externalHTTPSPort: 443
  externalGRPCSPort: 1986
  # Exposing metrics port in service is disabled by default
  # externalMetricsPort: 9090

  internalHTTPPort: 8080
  internalGRPCPort: 1985
  internalHTTPSPort: 8081
  internalGRPCSPort: 1986
  internalDistPort: 5151
  internalMetricsPort: 9090

  annotations: {}
  # loadBalancerIP:
  loadBalancerSourceRanges: []

## You have two options for exposing BuildBuddy with TLS over HTTPS and GRPCS.
## 1) Via the load balancer
##   - Point your DNS at the load balancer IP, you can get this by deploying and running:
##        echo `kubectl get --namespace default service buildbuddy -o jsonpath='{.status.loadBalancer.ingress[0].*}'`
##   - Enable config.ssl.enable_ssl and config.ssl.use_acme to automatically get configure letsencrypt certs
##   - Adding http and grpc host names to config.ssl.host_whitelist
##   - Wait a bit for dns propagation to happen and certs to be issued (this can take a while).
##
## 2) Ingress
##   - Enable ingress with ingress.enabled and ingress.sslEnabled
##   - Configure http and grpc host names with ingress.httpHost and ingress.grpcHost
##   - Enable cert manager with certmanager.enabled
##   - Update the cert manager email address in certmanager.emailAddress
##   - Before you deploy, install CRDs with:
##        kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.16.1/cert-manager.crds.yaml
##   - Point your DNS at the ingress controller IP, you can get this by deploying and running:
##        echo `kubectl get service my-release-ingress-controller -o jsonpath='{.status.loadBalancer.ingress[0].*}'`
##   - Wait a bit for dns propagation to happen and certs to be issued (this can take a while).

ingress:
  enabled: false
  class: "nginx"

  ## To enable SSL, either enable certmanager below or specify a
  ## clusterIssuer that's already installed in the cluster.
  sslEnabled: false
  #clusterIssuer: "letsencrypt-prod"

  httpHost: buildbuddy.example.com
  grpcHost: buildbuddy-grpc.example.com

  annotations: {}

  ## Nginx controller
  controller:
    extraVolumes:
      - name: client-ca-volume
        secret:
          secretName: buildbuddy-client-ca
    extraVolumeMounts:
      - name: client-ca-volume
        mountPath: /client-ca/ # Client CA must be mounted in the directory /client-ca/
        readOnly: true
    config:
      load-balance: "round_robin"
      ## The defaults here are low - this sets them for all servers.
      proxy-body-size: "0"
      ignore-invalid-headers: "false"
      server-snippet: |
        grpc_socket_keepalive on;
        grpc_read_timeout 7200s;
        grpc_send_timeout 7200s;
        client_body_timeout 7200s;
        client_body_buffer_size 4m;
        http2_idle_timeout 7200s;
        http2_max_concurrent_streams 10000;
        http2_max_requests 100000;
      # This snippet handles pulling api keys from the cert or host and
      # shoving them into the $api_key variable / can be used in headers.
      http-snippet: |
        map $ssl_client_s_dn $cert_auth_type {
          default "";
          ~serialNumber=(?<api_key>[^,]*) "api_key_from_cert";
        }
        map $host $host_auth_type {
          default "";
          ~^(?<api_key>[^@]*)@ "api_key_from_host";
        }
        map $http_x_buildbuddy_api_key $api_key {
          "~." $http_x_buildbuddy_api_key;
        }
  # metrics:
  #   enabled: true
  #   service:
  #     annotations:
  #       prometheus.io/scrape: "true"
  #       prometheus.io/port: "10254"

  ## AWS's LoadBalancer(ALB) Controller
  ## Documentation: https://kubernetes-sigs.github.io/aws-load-balancer-controller
  ## Configuration steps:
  ##   - Install `aws-load-balancer-controller` into your cluster
  ##   - Disable nginx-controller with `ingress.controller.enabled: false`
  ##   - Enable ingress by setting `ingress.enabled: true`
  ##   - Configure `ingress.class: "alb"` for the controller to aware of the Ingress
  ##   - Depending on what type is your Service, the chart will provision different things:
  ##     + LoadBalancer: ALB controller will provision a legacy Elastic Load Balancer corresponding to your Service directly.
  ##     + NodePort: ALB controller will provision a set of Load Balancer corresponding to your Ingress configurations.
  ##       Note: Serving GRPC requests through ALB requires an SSL connection so you need to set `ingress.alb.certificateArn`
  ##             to a valid certificate ARN on AWS. Example below.
  # alb:
  #   certificateArn: "arn:aws:acm:us-west-2:xxxxx:certificate/xxxxxxx"

certmanager:
  ## Before enabling certmanager, make sure to install the CRDs on your cluster with:
  ## kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.16.1/cert-manager.crds.yaml
  enabled: false
  ## If these CRDs are not installed, you will get the following error message:
  ## Error: unable to build kubernetes objects from release manifest: unable to recognize "": no matches for kind "ClusterIssuer" in version "cert-manager.io/v1alpha2"
  ##
  ## Email address to use for letsencrypt certs
  emailAddress: your-email@gmail.com

## Either enable mysql here, or use your own data_source connection string
## in the config block below.
mysql:
  enabled: false
  mysqlRootPassword: ""
  mysqlUser: ""
  mysqlPassword: ""
  mysqlDatabase: buildbuddy

  resources:
    requests:
      cpu: "2"
      memory: "4Gi"

  configurationFiles:
    mysql.cnf: |-
      [mysqld]
      max_connections=4000
  repository: mysql
  imageTag: "8.0.18"
  # metrics:
  #   enabled: true
  #   annotations:
  #     prometheus.io/scrape: "true"
  #     prometheus.io/port: "9104"

redis:
  enabled: false

# Autoscaler for buildbuddy-app instances
#
# This will NOT work if distributed.enabled is true.
# See https://github.com/buildbuddy-io/buildbuddy-helm/issues/23 for more info.
autoscaler:
  enabled: false
#   minReplicas: 3
#   maxReplicas: 100
#   averageCPU: 80
#   averageMemory: 80

# Required for autoscaling executors based on custom metrics
prometheus:
  enabled: false

prometheus-adapter:
  prometheus:
    url: http://{{ .Release.Name }}-prometheus-server
    port: 80
  rules:
    default: false
    custom:
      - seriesQuery: 'buildbuddy_remote_execution_queue_length{pod!="",namespace!=""}'
        resources:
          overrides:
            namespace: { resource: "namespace" }
            pod: { resource: "pod" }
        metricsQuery: sum (<<.Series>>{<<.LabelMatchers>>}) by (<<.GroupBy>>)
        name:
          matches: "^(.*)$"
          as: "${1}"

executor:
  enabled: false
#   replicas: 3
## Autoscaler for buildbuddy-executor instances
#   autoscaler:
#     enabled: true
#     minReplicas: 3
#     maxReplicas: 100
#     averageCPU: 90
#     averageMemory: 50
#     averageQueueLength: 5
#     ## Optional scaling behavior
#     ## https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#scaling-policies
#     # behavior:
#     #   scaleDown:
#     #     stabilizationWindowSeconds: 600
#     #     policies:
#     #     - type: Pods
#     #       value: 2
#     #       periodSeconds: 10
#     #     - type: Percent
#     #       value: 10
#     #       periodSeconds: 60
## The (optional) nodeSelector used for placing executors and apps in separate node pools/groups (useful when autoscaling executors)
#   nodeSelector:
#     ## GCP
#     cloud.google.com/gke-nodepool: my-executor-pool
#     ## AWS
#     eks.amazonaws.com/nodegroup: my-executor-pool
#   config:
#     executor:
#       # If using multiple app replicas, point this at your ingress grpcHost with grpc(s):// prefix and port.
#       app_target: "grpc://buildbuddy-enterprise:1985"

# NOTE: These acts as the default values for the config.yaml file read by the
# buildbuddy server itself. You can override the config object just like any
# Helm template value. Since it is an object, the object you provide will merge
# with these defaults.
config:
  app:
    # build_buddy_url: "https://buildbuddy.example.com"
    # events_api_url: "grpcs://buildbuddy-grpc.example.com"
    # cache_api_url: "grpcs://buildbuddy-grpc.example.com"
    # default_redis_target: redis:6379 # ENTERPRISE ONLY
    default_to_dense_mode: false
  database:
    ## Use an external database connection string here, or deploy mysql using this chart with mysql.enabled
    ## mysql:     "mysql://<USERNAME>:<PASSWORD>@tcp(<HOST>:3306)/<DATABASE_NAME>"
    ## sqlite:    "sqlite3:///tmp/buildbuddy-enterprise.db"
    data_source: "" # Either set this or mysql.enabled, not both!
  storage:
    ttl_seconds: 2592000 # 30 days
    # ttl_seconds: 0  # Do not TTL build events.
    chunk_file_size_bytes: 3000000 # 3 MB
    disk:
      root_directory: "/data/buildbuddy/blobs/" # Enabled disk.data volume above to use
    # gcs:
    #   bucket: "buildbuddy_blobs"
    #   project_id: "my-gc-project"
    #   credentials_file: "my-volume/my-google-credentials.json"
  cache:
    disk:
      root_directory: "/data/buildbuddy/cache/" # Enabled disk.data volume above to use
    max_size_bytes: 10000000000 # 10 GB
  ssl:
    enable_ssl: false
    client_ca_cert_file: "" # DO NOT EDIT: Automatically configured using certmanager
    client_ca_key_file: "" # DO NOT EDIT: Automatically configured using certmanager
    # use_acme: true # Use acme for letsencrypt ssl certs if you point your domains at LoadBalancer without an ingress.
    # host_whitelist: # Host whitelist to enable acme for
    #   - buildbuddy.example.com
    #   - buildbuddy-grpc.example.com
  # org:
  #   name: Acme Corp
  #   domain: acme.com
  # integrations:
  #   slack:
  #     webhook_url: "https://hooks.slack.com/services/MY/WEBHOOK/URL"
  # auth: # ENTERPRISE ONLY
  #   enable_anonymous_usage: false
  #   oauth_providers:
  #     - issuer_url: "https://accounts.google.com" # OIDC compatible discovery url
  #       client_id: "MY_CLIENT_ID"
  #       client_secret: "MY_CLIENT_SECRET"
  # api: # ENTERPRISE ONLY
  #   enable_api: true
  # github: # ENTERPRISE ONLY
  #   client_id: MY_CLIENT_ID
  #   client_secret: MY_CLIENT_SECRET
  remote_execution:
    enable_remote_exec: false

grafana:
  enabled: false
  grafana.ini:
    server:
      domain: "{{ .Values.ingress.httpHost }}"
      root_url: "%(protocol)s://%(domain)s:%(http_port)s/grafana/"
      serve_from_sub_path: true
      cookie_secure: false
    dashboards:
      default_home_dashboard_path: /var/lib/grafana/dashboards/default/buildbuddy-dashboard.json
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: "default"
          orgId: 1
          folder: ""
          type: file
          disableDeletion: true
          editable: false
          options:
            path: /var/lib/grafana/dashboards/default
  dashboardsConfigMaps:
    default: buildbuddy-dashboard
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          uid: prom
          url: http://{{ .Release.Name }}-prometheus-server
          access: proxy
          isDefault: true

## Additional env vars
extraEnvVars: []

## Additional init containers
extraInitContainers: []

# Add additional volumes and mounts
extraVolumes: []
extraVolumeMounts: []

## Container image that is used to ping mysql until it's up.
initContainerImage:
  repository: busybox
  tag: latest
  imagePullPolicy: IfNotPresent

## Affinity (optional) could be used to configured 'app' pods to be deployed in separate nodes,
## or nodes with a certain type of storage volume available.
affinity: {}
