## Default values for BuildBuddy Enterprise.
## This is a YAML-formatted file.
## Declare variables to be passed into your templates.

## The BuildBuddy Executor container image to use.
image:
  repository: gcr.io/flame-public/buildbuddy-executor-enterprise
  tag: enterprise-v2.202.0
  imagePullPolicy: IfNotPresent

## The number of executor replicas to run.
replicas: 3

## The name of the executor pool for this deployment.
# poolName: high-memory-pool

## The nodeSelector used for placing executors and apps in separate node pools/groups (useful when autoscaling executors)
# nodeSelector:
#   # GCP
#   cloud.google.com/gke-nodepool: my-executor-pool
#   # AWS
#   eks.amazonaws.com/nodegroup: my-executor-pool

## The taints to schedule pods on (useful for mixed architecture clusters)
# tolerations:
#   - effect: NoSchedule
#     key: hardware_requirements
#     operator: Equal
#     value: x86_64

## The default resources to give the app.
resources:
  limits:
    cpu: "2"
    memory: "10Gi"
  requests:
    cpu: "1"
    memory: "5Gi"

## Configures what port the BuildBuddy executors operate on.
service:
  internalHTTPPort: 8080
  internalGRPCPort: 1985
  internalMetricsPort: 9090

# Required to enable autoscaling based on custom metrics
prometheus:
  enabled: false

prometheus-adapter:
  prometheus:
    url: http://{{ .Release.Name }}-prometheus-server
    port: 80
  rules:
    default: false
    custom:
      - seriesQuery: 'buildbuddy_remote_execution_queue_length{pod!="",namespace!=""}'
        resources:
          overrides:
            namespace: {resource: "namespace"}
            pod: {resource: "pod"}
        metricsQuery: sum (<<.Series>>{<<.LabelMatchers>>}) by (<<.GroupBy>>)
        name:
          matches: "^(.*)$"
          as: "${1}"

autoscaler:
  enabled: false
#   minReplicas: 3
#   maxReplicas: 100
#   averageQueueLength: 5
#   ## Optional scaling behavior
#   ## https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#scaling-policies
#   behavior:
#     scaleDown:
#       stabilizationWindowSeconds: 600
#       policies:
#       - type: Pods
#         value: 2
#         periodSeconds: 10
#       - type: Percent
#         value: 10
#         periodSeconds: 60
#   ## Optional custom metrics
#   metrics:
#   - type: Pods
#     pods:
#       metric:
#         name: my_custom_metric
#       target:
#         type: AverageValue
#         averageValue: 5

## Custom deployment strategy
strategy:
  type: RollingUpdate
#   rollingUpdate:
#     maxSurge: 0
#     maxUnavailable: 1

podDisruptionBudget:
  enabled: false
  spec:
    ## Only one of minAvailable or maxUnavailable may be specified
    # maxUnavailable: 40%
    minAvailable: 60%

# NOTE: These acts as the default values for the config.yaml file read by the
# buildbuddy server itself. You can override the config object just like any
# Helm template value. Since it is an object, the object you provide will merge
# with these defaults.
config:
  executor:
    # app_target: "grpcs://your.buildbuddy.install:443" # auto populated if deploying via buildbuddy-enterprise chart
    root_directory: "/buildbuddy/remotebuilds/"
    local_cache_directory: "/buildbuddy/filecache/"
    local_cache_size_bytes: 5000000000  # 5GB
    ## Use oci to spin up 'runner' child containers inside executor container.
    enable_oci: true
    default_isolation_type: oci

# Configuration for Deployment Rollout Staggering
# The 'minReadySeconds' config delays the rollout process during a RollingUpdate.  This delay allows the
# new Executor time for initialization, which includes picking up new actions, downloading necessary
# container images and inputs.  As a result, it prevents the immediate replacement of the old Executor.
# By setting this config, the impact of 'helm upgrade' on ongoing operations is minimized, ensuring a
# smoother transition between different chart releases.
#
# minReadySeconds: 300

## Additional env vars
extraEnvVars: []

## Probe configuration
## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
## Example:
##   extraLivenessProbeConfigs:
##     timeoutSeconds: 10
##   extraReadinessProbeConfigs:
##     timeoutSeconds: 10
extraLivenessProbeConfig: {}
extraReadinessProbeConfig: {}

## Additional init containers
extraInitContainers: []

## Additional containers
extraContainers: []

## Path on the node to mount 'executor-data' volume.
## If not set, 'executor-data' will use emptyDir.
# executorDataVolumeHostPath: ""

## Path on the node to mount Podman 'containers-lib' volume,
## which is mounted to /var/lib/containers inside the Executor pod.
##
## If not set, 'containers-lib' will use emptyDir.
# podmanGraphRootVolumeHostPath: ""

## Path on the node to mount Podman 'containers-run' volume,
## which is mounted to /run/containers inside the Executor pod.
##
## If not set, 'containers-run' will use emptyDir.
# podmanRunRootVolumeHostPath: ""

# Add additional volumes and mounts
extraVolumes: []
extraVolumeMounts: []

## Affinity (optional) could be used to configured 'executor' pods to be deployed
## on nodes with a certain type of storage volume or hardware available.
affinity: {}

## Optionally, set a different service account for the executor pods. Setting
## this is not required for normal 'executor' operation; the `default` SA is
## sufficient. It is available in case you need to grant the pods additional
## permissions via SA. Note that the chart does not create the SA, so you should
## create it on your own.
# serviceAccount:
#   name: default

## TopologySpreadConstraints for pod assignment
## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/
##
## Example: The Executor pods should be evenly spread across zones
# topologySpreadConstraints:
#   - maxSkew: 1
#     topologyKey: zone
#     whenUnsatisfiable: DoNotSchedule
#     labelSelector:
#       matchLabels:
#         foo: bar
topologySpreadConstraints: []
